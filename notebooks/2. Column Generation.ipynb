{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Notebook setup\n",
    "# ============================================================\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Control figure size\n",
    "interactive_figures = False\n",
    "if interactive_figures:\n",
    "    # Normal behavior\n",
    "    %matplotlib widget\n",
    "    figsize=(9, 3)\n",
    "else:\n",
    "    # PDF export behavior\n",
    "    figsize=(14, 4)\n",
    "\n",
    "from util import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Column Generation for Better Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scalability Issues\n",
    "\n",
    "**The main drawback of the cycle formulation is the limited scalability**\n",
    "\n",
    "* The number of cycles grows with the graph size as $O(n^{\\text{max length}})$\n",
    "* The enumeration becomes _more expensive_ and the model becomes _larger_\n",
    "\n",
    "**Both can quickly become major bottlenecks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Size 150, enumeration time\n",
      "CPU times: user 7.34 s, sys: 10 ms, total: 7.35 s\n",
      "Wall time: 7.35 s\n",
      "Number of cycles: 43206\n",
      ">>> Size 150, solution time\n",
      "CPU times: user 1.57 s, sys: 63.5 ms, total: 1.64 s\n",
      "Wall time: 1.65 s\n"
     ]
    }
   ],
   "source": [
    "pairs2, arcs2, aplus2 = util.generate_compatibility_graph(size=150, seed=2)\n",
    "print('>>> Size 150, enumeration time')\n",
    "%time cycles2 = util.find_all_cycles(aplus2, max_length=4, cap=None)\n",
    "print(f'Number of cycles: {len(cycles2)}')\n",
    "print('>>> Size 150, solution time')\n",
    "%time _, _, _ = util.cycle_formulation(pairs2, cycles2, tlim=10, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=big>\n",
    "    <small>Essentially, we have too many variables?</small><br>\n",
    "    How can we address this?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Incremental Addition of Variables\n",
    "\n",
    "**We'll see how we can _introduce variables as needed_**\n",
    "\n",
    "Let us assume we have an optimization problem over a non-negative variable:\n",
    "\n",
    "$$\n",
    "\\mathop{\\rm argmin}_{x  \\geq 0} f(x)\n",
    "$$\n",
    "\n",
    "* We assume that $f(x)$ is _convex_, which would make the problem easy\n",
    "* ...Except that $x$ is so _large-dimensional_ that we cannot scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We could obtain a solution quickly as follows:**\n",
    "\n",
    "* We restrict to a small subset $S$ of the components of $x$\n",
    "* ...We fix to 0 all other variables, i.e. $x_j = 0$ if $j \\notin S$\n",
    "* ...Then we find an optimum $x^*_S$ via any suitable approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=big>\n",
    "But how do we know whether $x^*_S$ is optimal?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pricing\n",
    "\n",
    "**We know that $x^*_S$ is optimal w.r.t. variables in $S$**\n",
    "\n",
    "For all the _other_ $x_j$, we can test a condition _after_ the problem is solved:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x^*_S)}{\\partial x_j} \\geq 0 \\qquad \\forall j \\notin S\n",
    "$$\n",
    "\n",
    "**We know that:**\n",
    "\n",
    "* The cost function is convex\n",
    "* All variables $\\notin S$ are forced to 0 $\\Rightarrow$ if we add them, they can only _increase_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Therefore:**\n",
    "\n",
    "* If $\\partial f(x^*_S) / \\partial x_j \\geq 0$, then adding the variable cannot be beneficial\n",
    "* If $\\partial f(x^*_S) / \\partial x_j < 0$, then adding the variable may improve the cost\n",
    "\n",
    "**This post-solution derivative check is sometimes called _pricing_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pricing for Incremental Variable Addition\n",
    "\n",
    "**So, we _a criterion_ to find which variables should be added**\n",
    "\n",
    "In principle, we could proceed as follows:\n",
    "\n",
    "* Choose $S$ and we solve over $x_S$\n",
    "* Loop over all $j \\notin S$ and check $\\partial f(x^*) / \\partial x_j$\n",
    "* Add the non-optimal variables to $S$ and repeat until $\\nabla_x f(x^*) \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**This is a nice method, but it has one potential weakness:**\n",
    "\n",
    "* If we need to  _enumerate_ to check $\\partial f(x^*) / \\partial x_j$\n",
    "* ...That may still take way too much time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **We need a way to do the derivative check more efficiently**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## From Variable Addition to Variable Generation\n",
    "\n",
    "**Let's focus on decision variables representing complex entities**\n",
    "\n",
    "...Which can be constructed based on simpler building blocks\n",
    "\n",
    "* E.g. cycles including several _nodes_\n",
    "* E.g. routes including several _arcs_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**We can formalize this situation as follows:** \n",
    "\n",
    "$$\n",
    "x_j \\equiv g(y_j)\n",
    "$$\n",
    "\n",
    "Given a variable $y_j$ that specifies which building blocks are used:\n",
    "\n",
    "* E.g. which nodes are included in the $j$-th cycle\n",
    "* E.g. which arcs are included in the $j$-th route\n",
    "\n",
    "...The function $g(y)$ specifies how a$x_j$ is built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pricing Problem\n",
    "\n",
    "**In these cases, we can avoid enumeration by using optimization**\n",
    "\n",
    "* First, we compute _in closed form_ the derivative $\\partial f(x^*_S) / \\partial g(y)$\n",
    "* Then we solve the _pricing problem_:\n",
    "\n",
    "$$\n",
    "y^* = \\mathop{\\rm argmin}_{y} \\frac{\\partial f(x^*_S)}{\\partial g(y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The result is the \"recipe\" $y^*$ for the best possible variable $g(y^*)$**\n",
    "\n",
    "* I.e. the set of nodes leading to the best cycle\n",
    "* I.e. the set of arcs leading to the best route\n",
    "\n",
    "...And we can check the corresponding partial derivative as before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Column Generation\n",
    "\n",
    "**Let's revisit the process with this change**\n",
    "\n",
    "* Choose $S$ and solve a _restricted master problem_ over $x_S$\n",
    "* Solve the pricing problem:\n",
    "$$\n",
    "y^* = \\mathop{\\rm argmin}_{y} \\frac{\\partial f(x^*_S)}{\\partial g(y)}\n",
    "$$\n",
    "* If $\\partial f(x^*_S) / g(y^*) \\geq 0$: the solution is optimal\n",
    "* Oherwise: add $g(y^*)$ to the set variables and repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This process is called variable generation, or more often _column generation_**\n",
    "\n",
    "...Because in Linear Programming variables are associated to columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=big>\n",
    "    <small>The cycle formulation has a lot of variables...</small><br>\n",
    "    Can we use this method for our use case?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP\n",
    "\n",
    "**We can try to use it for the _relaxed_ cycle formulation:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min & - \\sum_{j=1}^n w_j x_j \\\\\n",
    "\\text{s.t. } & \\sum_{j=1}^n a_{ij} x_j \\leq 1 & \\forall i = 1..m \\\\\n",
    "& x_j \\geq 0 & \\forall j = 1..n\n",
    "\\end{align}$$\n",
    "\n",
    "* All _integrality_ constraints are _relaxed_, making the problem _convex_\n",
    "* Note that $x_j \\leq 1$ is implied by the other problem constraints\n",
    "* We also changed the optimization direction to match the CG theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP\n",
    "\n",
    "**Now we need to specify several things**\n",
    "\n",
    "* How to solve the restricted _master problem_\n",
    "* How to compute the _partial derivative_ $\\partial f(x^*_S) / \\partial x_j$\n",
    "* Which _basic decisions $y$_ to use for constructing a solution\n",
    "* How those decision affect _the variable buing built_, i.e. the $g(y)$ function\n",
    "* Finally, we need to define the _pricing problem_\n",
    "\n",
    "> **We'll tackle one step at a time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Restricted Master Problem\n",
    "\n",
    "**Solving this problem for a subset $S$ of variable is easy:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min & - \\sum_{\\color{red}{j \\in S}} w_j x_j \\\\\n",
    "\\text{s.t. } & \\sum_{\\color{red}{j \\in S}} a_{ij} x_j \\leq 1 & \\forall i = 1..m \\\\\n",
    "& x_j \\geq 0 & \\forall j = 1..n\n",
    "\\end{align}$$\n",
    "\n",
    "* Rather than setting all other variables to 0\n",
    "* We just restrict the summations\n",
    "\n",
    "This is _much cheaper_ in terms of memory usage and solution time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Partial Derivative\n",
    "\n",
    "**The tricky part is computing $\\partial f(x^*_S) / \\partial x_j$**\n",
    "\n",
    "At a first glance, this seems easy:\n",
    "\n",
    "* By differentiating:\n",
    "\n",
    "$$\n",
    "- \\sum_{j = 1}^n w_j x_j\n",
    "$$\n",
    "\n",
    "* We simply get:\n",
    "\n",
    "$$\n",
    "- w_j\n",
    "$$\n",
    "\n",
    "* However, unlike in our theoretical formulation\n",
    "* ...Our problem has _additional constraints_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Partial Derivative\n",
    "\n",
    "**In particular, we have node mutual exclusion**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min & - \\sum_{j = 1}^n w_j x_j \\\\\n",
    "\\text{s.t. } & \\color{red}{\\sum_{j = 1}^n a_{ij} x_j \\leq 1} & \\forall i = 1..m \\\\\n",
    "& x_j \\geq 0 & \\forall j = 1..n\n",
    "\\end{align}$$\n",
    "\n",
    "* In the optimal solution, some gradient component can be $> 0$\n",
    "* ...Because the constraint prevents from moving in that diretion\n",
    "\n",
    "> **How can we account for this?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Partial Derivative\n",
    "\n",
    "**Linear Programs satisfy _strong duality_**\n",
    "\n",
    "This means that the constraints can be _turned into cost terms_:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min \\mathcal{L}(x, \\lambda) = & - \\sum_{j = 1}^n w_j x_j + \\sum_{i=1}^m \\lambda_i \\left( \\sum_{j \\in S}^n a_{ij} x_j - 1 \\right) \\\\\n",
    "\\text{s.t. } & x_j \\geq 0  \\qquad \\forall j = 1..n\n",
    "\\end{align}$$\n",
    "\n",
    "The new cost function $\\mathcal{L}(x, \\lambda)$ is called a _Lagrangian_\n",
    "\n",
    "* It is possible to define _Lagrangian (or dual) multipliers $\\lambda_i$_\n",
    "* S.t. $\\nabla \\mathcal{L}$ behaves like a normal gradient for an optimal solution\n",
    "\n",
    "**In fact, all LP solvers are capable to returning those $\\lambda$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Partial Derivative\n",
    "\n",
    "**So, we differentiate $\\mathcal{L}$ rather than the original cost**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}(x^*_S)}{\\partial x_j} = -w_j + \\sum_{i=1}^m \\lambda_i^* a_{ij}\n",
    "$$\n",
    "\n",
    "* Where $\\lambda_i^*$ are the optimal multipliers for the $x^*_S$ solution\n",
    "* Again, they are computed automatically by the solver\n",
    "\n",
    "I.e. this derivative is for one specific solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This partial derivative is called a _reduced cost_**\n",
    "\n",
    "* Reduced costs can be computed by using standard formulas\n",
    "* ...But here we have derived them step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Building Variables\n",
    "\n",
    "**Now we need to specify how to build a variable, i. the $g(y)$ function**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\min & - \\sum_{j = 1}^n \\color{red}{w_j} x_j \\\\\n",
    "\\text{s.t. } & \\sum_{j = 1}^n \\color{red}{a_{ij}} x_j \\leq 1 & \\forall i = 1..m \\\\\n",
    "& x_j \\geq 0 & \\forall j = 1..n\n",
    "\\end{align}$$\n",
    "\n",
    "The basic decision $y_i$ consists in choosing whether to include node $i$\n",
    "\n",
    "* When we set $y_i = 1$, for a previously unused variable $x_j$\n",
    "* ...We increase $w_j$ by 1 and we set $a_{ij} = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Pricing Problem\n",
    "\n",
    "**The goal of the pricing problem is to minimize $\\partial f(x^*_S) / \\partial g(y)$**\n",
    "\n",
    "In practice this mean computeing the reduced cost:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x^*_S)}{\\partial x_j} = -w_j + \\sum_{i=1}^m \\lambda_i^* a_{ij}\n",
    "$$\n",
    "\n",
    "...Expressed _as a function of $y$_:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f(x^*_S)}{\\partial g(y)} = - \\underbrace{\\sum_{i=1}^m y_i}_{w_j} + \\sum_{i=1}^m \\lambda_i^* y_i\n",
    "$$\n",
    "\n",
    "* This is true since we can _decide_ which nodes to include"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CG for the KEP: Pricing Problem\n",
    "\n",
    "**Overall, our pricing problem is as follows:**\n",
    "\n",
    "$$\\begin{align}\n",
    "\\mathop{\\rm argmin} & \\sum_{i=1}^m y_i (- 1 + \\lambda_i^*) \\\\\n",
    "\\text{s.t. } & y \\text{ defines a cycle} \\\\\n",
    "& \\sum_{i=1}^m y_i \\leq C \\\\\n",
    "& y_i \\in \\{0, 1\\} & \\forall i = 1..m\n",
    "\\end{align}$$\n",
    "\n",
    "* Our selection nodes should have minimal weight\n",
    "* ...It should define a cycle\n",
    "* ...And it should not be too large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Column Generation\n",
    "\n",
    "**Overall, this is how CG is set up for Linear Programs**\n",
    "\n",
    "...Which are by far the most common application case:\n",
    "\n",
    "\n",
    "<center style=\"padding-top: 1em; padding-bottom: 1em\"><img src=\"assets/cg.png\" width=\"70%\"/></center>\n",
    "\n",
    "* After every MP iteration, we obtain the dual multiplier\n",
    "* ...Then we solve the pricing problem to obtain the best possible variable\n",
    "* If the corresponging reduced cost is $\\geq 0$, we proved optimality\n",
    "* Otherwise, we keep on looping"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "rise": {
   "center": false,
   "enable_chalkboard": true,
   "transition": "fade"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
